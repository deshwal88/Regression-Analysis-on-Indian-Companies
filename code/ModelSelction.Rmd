---
title: "Model Selection and Prediction"
author: "Kanishk Deshwal"
date: "`r Sys.Date()`"
output: html_document
---

```{r}

companies <- read.csv("../data/indian_companies_transformed.csv", skipNul = TRUE)
data$INDUSTRY = factor(data$INDUSTRY)
data$INDIA.HQ = factor(data$INDIA.HQ)

```

```{r}
lmod = lm(PACKAGE ~ YEARS.OLD + INDUSTRY + INDIA.HQ + TOTAL_EMPLOYEES + BRANCHES + I(RATING^2) + REVIEWS, data)
summary(lmod)
```

**
Checking for Inflated Variance - Stabilizing $\beta$
**

Now, checking for multicoliearity using correlation matrix and calculating Condition number.

```{r}

X = model.matrix(lmod)
cat_vars = c("YEARS.OLD","RATING","REVIEWS","BRANCHES","TOTAL_EMPLOYEES")
pairs(data[cat_vars], labels = cat_vars, main="Pairwise scatter plot of data")
cat("Correlation Matrix:\n")
cor(X[, c(2, 19, 20, 21, 22)])

```

```{r}

n = dim(X)[1]
p = dim(X)[2]

XtX = t(X)%*%X
lambda = eigen(XtX)$values
cat("\u03BB =", lambda, "\n")

K = sqrt(lambda[1]/lambda[p])
cat("Condition number is given by K = ", K)
```
High Condition Number states that this is an extreme case of Multi-collinearity which signals that the variance of estimators may be inflated. To further confirm this Variance Inflation Factor must be calculated.

```{r}
library(faraway)
v = vif(lmod)
print(v)
cat("Average VIF for predictors: ", sum(v)/p, "\n")
cat("Maximum value of VIF: ", max(v))
```
Average and Maximum values of VIF depicts that the variance of each estimate is not inflated (atleast to a value that posses a concern). The huge condition number is due to design-matrix structure (dummy coding), not harmful collinearity in the regression coefficients. Furthermore, the pairwise correlations among predictors are all below 0.6, which is additional evidence that the independent variables do not exhibit strong linear relationships.

**
Model Selection  
**

As the number of parameters p<<n, a balanced criterion is required to balance predictive power and complexity of the model. Thus, AIC is ideal criterion with Hybrid(Both) search.

```{r warning=FALSE}

null = lm(PACKAGE ~ 1, na.omit(data))
full = lm(PACKAGE ~ ., na.omit(data))
out = step(null, scope = list(lower=~1, upper=formula(full)), direction = "both", trace = FALSE)
cat("Order of parameters: ",out$anova[,1])
```

Here, Years.OLD and TOTAL_EMPLOYEES were dropped from the model as a result.

```{r}
plot(out$anova[,6], xlab="Number of Params", ylab="AIC", main="AIC vs p")
```
  
Looking at the AIC vs Index plot, we see that AIC remains almost constant after 4. Replacing AIC with more strict criterion, BIC.

```{r}
out = step(null, scope = list(lower=~1, upper=formula(full)), direction = "both", k=log(n), trace = FALSE)
cat("Order of parameters:", out$anova[,1], "\n")
cat("Adjusted R-squared: ", summary(out)$adj.r.squared)
summary(out)
```

**
Prediction
**

Lets take examples of Indian companies with:   
  
Number of Reviews = 10000 (popular)  
Rating = 3 (average)   
Branches = 5   
Headquarter = Bangalore / Bengaluru  
Industry = IT Services & Consulting

```{r echo=FALSE }

x_new = data.frame(REVIEWS = log(5000), BRANCHES = log(5), RATING=log(3),
                   INDIA.HQ=factor("Bangalore / Bengaluru", levels = levels(data$INDIA.HQ)), 
                   INDUSTRY=factor("IT Services & Consulting", levels = levels(data$INDUSTRY)))

y = predict(out, newdata = x_new)
cat("Average monthly package predicted: ", round(exp(y), 2), "L")

```

Number of Reviews = 10000 (popular)    
Rating = 3 (average)    
Branches = 5   
Headquarter = Bangalore / Bengaluru  
Industry = Education & Training  

```{r echo=FALSE}

x_new = data.frame(REVIEWS = log(5000), BRANCHES = log(5), RATING=log(3),
                   INDIA.HQ=factor("Bangalore / Bengaluru", levels = levels(data$INDIA.HQ)), 
                   INDUSTRY=factor("Education & Training", levels = levels(data$INDUSTRY)))

y = predict(out, newdata = x_new)
cat("Average monthly package predicted: ", round(exp(y), 2), "L")

```